# Known Issues - API DJEN (CNJ)

**Prop√≥sito:** Documentar particularidades, armadilhas e erros conhecidos da API DJEN para evitar repeti√ß√£o de erros em desenvolvimento futuro.

**√öltima atualiza√ß√£o:** 2025-11-22

---

## ‚ö†Ô∏è CRITICAL - Leia antes de modificar c√≥digo de API

Este documento cont√©m **li√ß√µes aprendidas** de erros reais cometidos durante desenvolvimento. **Sempre consulte este arquivo** antes de escrever c√≥digo que interage com a API DJEN.

---

## 1. Pagina√ß√£o Autom√°tica - N√ÉO use `offset`

### ‚ùå **ERRO COMUM:**
```python
# ‚ùå ERRADO - offset n√£o existe!
publicacoes = downloader.baixar_api(
    tribunal='STJ',
    data='2025-11-20',
    limit=100,
    offset=200  # TypeError: unexpected keyword argument 'offset'
)
```

### ‚úÖ **CORRETO:**
```python
# ‚úÖ CORRETO - pagina√ß√£o autom√°tica com max_pages
publicacoes = downloader.baixar_api(
    tribunal='STJ',
    data='2025-11-20',
    limit=100,
    max_pages=1  # Limita a 1 p√°gina (opcional)
)
```

### üìù **Explica√ß√£o:**
- O m√©todo `baixar_api()` j√° faz pagina√ß√£o autom√°tica internamente
- Itera `page=1, 2, 3...` na URL at√© n√£o haver mais resultados
- **N√ÉO aceita** par√¢metro `offset` (estilo SQL)
- Use `max_pages` para limitar quantas p√°ginas baixar

### üîç **Assinatura correta:**
```python
def baixar_api(
    self,
    tribunal: str,
    data: str,
    limit: int = 100,
    max_pages: Optional[int] = None
) -> List[PublicacaoRaw]
```

**Fonte:** `src/downloader.py:209-215`

---

## 2. Rate Limiting - Janela Deslizante (21 req/janela)

### ‚ö†Ô∏è **LIMITE REAL DA API:**
- **21 requisi√ß√µes** por janela de ~5 segundos
- HTTP 429 ap√≥s exceder
- Header `Retry-After: 2` indica tempo de espera

### ‚ùå **ERRO COMUM:**
```python
# ‚ùå ERRADO - Delay fixo n√£o respeita janela
time.sleep(2)  # Simplista, n√£o funciona em alta taxa
```

### ‚úÖ **CORRETO:**
```python
# ‚úÖ CORRETO - Rate limiting adaptativo (implementado)
downloader = DJENDownloader(
    data_root=DATA_ROOT,
    requests_per_minute=144,  # Taxa sustent√°vel
    adaptive_rate_limit=True  # Janela deslizante autom√°tica
)
```

### üìù **Explica√ß√£o:**
- API usa **janela deslizante**, n√£o contador fixo
- 21 requests em 5s ‚Üí HTTP 429
- Sistema atual usa buffer conservador (12 req/5s) para confiabilidade
- Retry autom√°tico com `Retry-After` header

**Fonte:** `diagnostico_performance.py:196-207`, `src/downloader.py:_check_rate_limit()`

---

## 3. Database - Problema N+1 com Commits

### ‚ùå **ERRO COMUM:**
```python
# ‚ùå ERRADO - Commit em cada INSERT (535x mais lento!)
for pub in publicacoes:
    inserir_publicacao(conn, pub)
    conn.commit()  # N+1 commits = MUITO LENTO
```

### ‚úÖ **CORRETO:**
```python
# ‚úÖ CORRETO - Batch commits (implementado)
BATCH_SIZE = 100
for i, pub in enumerate(publicacoes, start=1):
    inserir_publicacao(conn, pub)

    if i % BATCH_SIZE == 0:
        conn.commit()  # Commit a cada 100

conn.commit()  # Commit final
```

### üìù **Explica√ß√£o:**
- SQLite √© single-writer, cada commit faz fsync no disco
- Commit individual: 117 writes/sec
- Batch commits (100): 62,894 writes/sec (**535x speedup**)

**Fonte:** `diagnostico_performance.py:64-79`, `scheduler.py:processar_publicacoes()`

---

## 4. Deduplica√ß√£o - Hash SHA256 (n√£o ID da API)

### ‚ö†Ô∏è **IMPORTANTE:**
API pode retornar mesma publica√ß√£o com IDs diferentes em requests subsequentes.

### ‚ùå **ERRO COMUM:**
```python
# ‚ùå ERRADO - ID da API n√£o garante unicidade
pub_id = item['id']  # Pode mudar entre requests
```

### ‚úÖ **CORRETO:**
```python
# ‚úÖ CORRETO - Hash do conte√∫do (implementado)
import hashlib
hash_conteudo = hashlib.sha256(texto_html.encode()).hexdigest()
```

### üìù **Explica√ß√£o:**
- Hash do `texto_html` garante unicidade real
- Detecta republica√ß√µes (mesmo conte√∫do, ID diferente)
- √çndice UNIQUE no DB: `CREATE UNIQUE INDEX idx_hash ON publicacoes(hash_conteudo)`

**Fonte:** `src/downloader.py:_gerar_hash()`, `scheduler.py:inicializar_banco()`

---

## 5. Timeout e Retry - Exponential Backoff

### ‚ùå **ERRO COMUM:**
```python
# ‚ùå ERRADO - Retry fixo sem backoff
for i in range(3):
    try:
        response = requests.get(url, timeout=5)
        break
    except Timeout:
        pass  # Retry imediato = pior
```

### ‚úÖ **CORRETO:**
```python
# ‚úÖ CORRETO - Exponential backoff (implementado)
for attempt in range(max_retries):
    try:
        response = requests.get(url, timeout=30)

        if response.status_code == 429:
            retry_after = int(response.headers.get('Retry-After', 2))
            time.sleep(retry_after)
            continue

        return response

    except Timeout:
        backoff = 2 ** attempt  # 1s, 2s, 4s
        time.sleep(backoff)
```

### üìù **Explica√ß√£o:**
- Timeout curto (5s) causa falhas desnecess√°rias
- Retry imediato sobrecarrega API
- Exponential backoff: 1s ‚Üí 2s ‚Üí 4s
- HTTP 429: usar `Retry-After` header

**Fonte:** `src/downloader.py:_fazer_requisicao()`

---

## 6. Filtro de Tipo - Case-Insensitive e Sem Acentos

### ‚ö†Ô∏è **IMPORTANTE:**
API retorna `tipo_publicacao` com varia√ß√µes:
- `"Ac√≥rd√£o"`, `"AC√ìRD√ÉO"`, `"  Ac√≥rd√£o\n"`
- Compara√ß√£o direta falha!

### ‚ùå **ERRO COMUM:**
```python
# ‚ùå ERRADO - Case-sensitive e com acentos
if pub['tipo_publicacao'] == 'Ac√≥rd√£o':  # Falha com 'AC√ìRD√ÉO'
```

### ‚úÖ **CORRETO:**
```python
# ‚úÖ CORRETO - Normaliza√ß√£o (implementado)
import unicodedata

def normalizar_tipo_publicacao(tipo: str) -> str:
    if not tipo:
        return ""
    # Remove acentos
    sem_acentos = unicodedata.normalize('NFD', tipo)
    sem_acentos = ''.join(c for c in sem_acentos if unicodedata.category(c) != 'Mn')
    return sem_acentos.lower().strip()

# Compara√ß√£o:
if normalizar_tipo_publicacao(pub['tipo_publicacao']) == 'acordao':
```

### üìù **Explica√ß√£o:**
- Normaliza: remove acentos, lowercase, trim
- `'Ac√≥rd√£o'` ‚Üí `'acordao'`
- `'AC√ìRD√ÉO  '` ‚Üí `'acordao'`
- Compara√ß√£o robusta

**Fonte:** `scheduler.py:normalizar_tipo_publicacao()`, `scheduler.py:processar_publicacoes()`

---

## 7. Payload Size - N√ÉO √© Gargalo (<100KB)

### ‚úÖ **VALIDADO:**
- Payload m√©dio: ~50-100KB por request
- JSON serialization: **n√£o √© gargalo** (confirmado por profiling)
- Parsing HTML: ~10ms (5% do tempo total)

### üìù **Conclus√£o:**
- **N√ÉO otimize** JSON parsing/serialization (ganho marginal)
- **Foco:** Rate limiting e batch commits (ganhos 10x e 500x)

**Fonte:** `diagnostico_performance.py:48-61`, `PROFILING-DETALHADO-V2.txt:98-100`

---

## 8. Connection Pooling - N√ÉO √© Necess√°rio

### ‚ö†Ô∏è **IMPORTANTE:**
SQLite √© **single-writer**, connection pooling n√£o ajuda.

### ‚ùå **ERRO COMUM:**
```python
# ‚ùå DESNECESS√ÅRIO - SQLite single-writer
from sqlalchemy import create_engine
engine = create_engine('sqlite:///db.sqlite', pool_size=10)
```

### ‚úÖ **CORRETO:**
```python
# ‚úÖ CORRETO - Conex√£o √∫nica (implementado)
import sqlite3
conn = sqlite3.connect('publicacoes.db')
# Uma conex√£o por processo √© suficiente
```

### üìù **Explica√ß√£o:**
- SQLite permite apenas 1 escritor simult√¢neo
- Connection pool n√£o melhora throughput
- Batch commits sim (535x speedup)

**Fonte:** An√°lise de diagn√≥stico, CLAUDE.md

---

## 9. Dados de Teste - Usar Datas Recentes

### ‚ö†Ô∏è **IMPORTANTE:**
Datas antigas (>30 dias) frequentemente retornam 0 publica√ß√µes.

### ‚ùå **ERRO COMUM:**
```python
# ‚ùå ERRADO - Testa com datas antigas vazias
data_teste = '2024-01-01'  # Provavelmente vazio
```

### ‚úÖ **CORRETO:**
```python
# ‚úÖ CORRETO - Testa com dados recentes
from datetime import datetime, timedelta
data_teste = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')
```

### üìù **Explica√ß√£o:**
- API ret√©m dados recentes com mais consist√™ncia
- Testes com datas >30 dias podem retornar vazio (n√£o erro)
- Para profiling: variar √∫ltimos 20 dias

**Fonte:** `profile_detalhado.py:70-71`, `PROFILING-DETALHADO-V2.txt:55-74`

---

## 10. Logging - Debug vs Info vs Warning

### ‚úÖ **PADR√ÉO ATUAL:**

```python
# DEBUG - Detalhes t√©cnicos (batch commits, rate limit)
logger.debug(f"Batch commit: {i}/100 processadas")

# INFO - Eventos importantes (in√≠cio download, conclus√£o)
logger.info(f"[STJ] Baixando publica√ß√µes via API - 2025-11-20")

# WARNING - Situa√ß√µes anormais n√£o cr√≠ticas (HTTP 429, timeout)
logger.warning(f"HTTP 429 - Aguardando 2s antes de retry")

# ERROR - Falhas cr√≠ticas (timeout ap√≥s retries)
logger.error(f"Timeout ap√≥s 3 tentativas")
```

### üìù **Orienta√ß√£o:**
- **DEBUG:** Usado para troubleshooting, desabilitado em produ√ß√£o
- **INFO:** Progresso normal, sempre vis√≠vel
- **WARNING:** Algo inesperado mas recuper√°vel
- **ERROR:** Falha cr√≠tica, requer aten√ß√£o

**Fonte:** `scheduler.py`, `src/downloader.py`

---

## 11. Headers HTTP - M√≠nimos S√£o Mais R√°pidos ‚ö°

### ‚úÖ **DESCOBERTA (2025-11-22):**
Headers m√≠nimos resultam em **7% menos lat√™ncia** (269ms vs 288ms default).

### ‚ùå **ERRO COMUM:**
```python
# ‚ùå DESNECESS√ÅRIO - Headers verbosos aumentam lat√™ncia
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0...) Chrome/120...',
    'Accept': 'application/json',
    'Accept-Encoding': 'gzip, deflate, br',
    'Accept-Language': 'pt-BR,pt;q=0.9,en-US;q=0.8'
}
```

### ‚úÖ **CORRETO:**
```python
# ‚úÖ CORRETO - Headers m√≠nimos (mais r√°pido)
headers = {
    'Accept': 'application/json'
}
# Resultado: 269ms avg (vs 288ms default)
```

### üìù **Explica√ß√£o:**
- Teste de 5 configura√ß√µes (100 requisi√ß√µes)
- **Minimal** venceu: 269ms avg (min 236ms, max 299ms)
- Browser-like headers: 273-288ms avg
- Ganho: **7% speedup** apenas removendo headers desnecess√°rios

**Fonte:** `tests/api/test_headers_impact.py`, `data/diagnostics/headers_impact.json`

---

## 12. Rate Limit Real - 198 req/min (n√£o 144) üöÄ

### ‚úÖ **DESCOBERTA CR√çTICA (2025-11-22):**
Limite real da API = **20 req/window** ‚Üí ~**198 req/min**

### ‚ö†Ô∏è **IMPORTANTE:**
Sistema atual usa **144 req/min**, mas pode subir para **180 req/min** com seguran√ßa.

### üìä **Evid√™ncias:**
```
HTTP 429 recebido ap√≥s 58 requisi√ß√µes em 17.6s
Taxa observada: 198 req/min
Header: X-RateLimit-Limit: 20 (req por janela)
Retry-After: 1s
```

### üí° **Recomenda√ß√£o:**
```python
# Configura√ß√£o √≥tima (margem de seguran√ßa 10%)
downloader = DJENDownloader(
    data_root=DATA_ROOT,
    requests_per_minute=180,  # Era 144 (pode subir!)
    adaptive_rate_limit=True,
    max_retries=3
)
```

### üìà **Ganho Potencial:**
- Taxa atual: 144 req/min
- Taxa √≥tima: 180 req/min
- **Speedup: 1.25x** (25% mais r√°pido)

**Fonte:** `diagnostico_performance.py:290-330`, `DIAGNOSTICO_PERFORMANCE.md`

---

## üìö REFER√äNCIAS R√ÅPIDAS

| Conceito | Arquivo | Linha |
|----------|---------|-------|
| Assinatura `baixar_api()` | `src/downloader.py` | 209-215 |
| Rate limiting adaptativo | `src/downloader.py` | `_check_rate_limit()` |
| Batch commits | `scheduler.py` | `processar_publicacoes()` |
| Normaliza√ß√£o tipo | `scheduler.py` | `normalizar_tipo_publicacao()` |
| Hash deduplica√ß√£o | `src/downloader.py` | `_gerar_hash()` |
| HTTP 429 handling | `src/downloader.py` | `_fazer_requisicao()` |

---

## üîÑ HIST√ìRICO DE UPDATES

| Data | Issue | Descri√ß√£o |
|------|-------|-----------|
| 2025-11-21 | #1 | Cria√ß√£o inicial - `offset` n√£o existe |
| 2025-11-21 | #2 | Rate limiting janela deslizante |
| 2025-11-21 | #3 | Problema N+1 batch commits |
| 2025-11-21 | #4 | Normaliza√ß√£o tipo case-insensitive |
| 2025-11-22 | #5 | Headers m√≠nimos s√£o 7% mais r√°pidos |
| 2025-11-22 | #6 | Rate limit real = 198 req/min (pode subir de 144‚Üí180) |

---

## ‚úÖ CHECKLIST - Antes de Modificar C√≥digo de API

- [ ] Li este documento completo
- [ ] Verifiquei assinatura do m√©todo `baixar_api()`
- [ ] N√£o usei par√¢metro `offset` (n√£o existe!)
- [ ] Rate limiting adaptativo est√° habilitado
- [ ] Commits s√£o em batch (n√£o individual)
- [ ] Retry tem exponential backoff
- [ ] Filtros de tipo s√£o normalizados
- [ ] Testes usam datas recentes (<30 dias)
- [ ] Logging usa n√≠veis adequados

---

**√öltima revis√£o:** 2025-11-22
**Maintainer:** Development Team
**Contato:** Ver CLAUDE.md

---

**‚ö†Ô∏è IMPORTANTE:** Este documento deve ser atualizado sempre que novos issues/particularidades da API forem descobertos.
