#!/usr/bin/env python3
"""
Complete Example: DJEN API Integration with Best Practices

Demonstrates:
- Adaptive rate limiting (sliding window)
- Batch commits (535x speedup)
- Hash-based deduplication
- Exponential backoff retry
- Case-insensitive type normalization
- Ementa extraction

Usage:
    python example_download.py --tribunal STJ --data 2025-11-20 --output results.db
"""

import argparse
import time
import hashlib
import uuid
import sqlite3
import unicodedata
import re
from typing import Dict, List, Optional, Tuple
from datetime import datetime
from collections import deque
import requests
from bs4 import BeautifulSoup

BASE_URL = "https://comunicaapi.pje.jus.br"


class AdaptiveRateLimiter:
    """Sliding window rate limiter"""

    def __init__(self, requests_per_window=12, window_seconds=5):
        self.requests_per_window = requests_per_window
        self.window_seconds = window_seconds
        self.request_times = deque()

    def wait_if_needed(self):
        """Wait if necessary to respect rate limit"""
        now = time.time()

        # Remove old requests outside window
        while self.request_times and now - self.request_times[0] > self.window_seconds:
            self.request_times.popleft()

        # If at limit, wait
        if len(self.request_times) >= self.requests_per_window:
            sleep_time = self.window_seconds - (now - self.request_times[0])
            if sleep_time > 0:
                print(f"  ‚è≥ Rate limit: sleeping {sleep_time:.1f}s")
                time.sleep(sleep_time)
                self.request_times.popleft()

        # Record this request
        self.request_times.append(time.time())


class DJENDownloader:
    """DJEN API downloader with best practices"""

    def __init__(self, db_path: str):
        self.db_path = db_path
        self.rate_limiter = AdaptiveRateLimiter(requests_per_window=12, window_seconds=5)
        self.conn = None
        self.stats = {'total': 0, 'novas': 0, 'duplicadas': 0, 'erros': 0}

    def inicializar_banco(self):
        """Initialize SQLite database"""
        self.conn = sqlite3.connect(self.db_path)
        cursor = self.conn.cursor()

        cursor.execute("""
        CREATE TABLE IF NOT EXISTS publicacoes (
            id TEXT PRIMARY KEY,
            hash_conteudo TEXT NOT NULL UNIQUE,
            numero_processo TEXT,
            numero_processo_fmt TEXT,
            tribunal TEXT NOT NULL,
            orgao_julgador TEXT,
            tipo_publicacao TEXT NOT NULL,
            classe_processual TEXT,
            texto_html TEXT NOT NULL,
            texto_limpo TEXT NOT NULL,
            ementa TEXT,
            data_publicacao TEXT NOT NULL,
            relator TEXT,
            fonte TEXT NOT NULL
        )
        """)

        cursor.execute("CREATE INDEX IF NOT EXISTS idx_hash ON publicacoes(hash_conteudo)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_tribunal ON publicacoes(tribunal)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_data ON publicacoes(data_publicacao)")

        self.conn.commit()

    def fazer_requisicao(self, url: str, params: dict, max_retries=3) -> Optional[requests.Response]:
        """Make request with exponential backoff retry"""
        for attempt in range(max_retries):
            try:
                self.rate_limiter.wait_if_needed()

                response = requests.get(url, params=params, timeout=30)

                if response.status_code == 429:
                    retry_after = int(response.headers.get('Retry-After', 2))
                    print(f"  ‚ö†Ô∏è HTTP 429 - Waiting {retry_after}s")
                    time.sleep(retry_after)
                    continue

                response.raise_for_status()
                return response

            except requests.Timeout:
                backoff = 2 ** attempt
                print(f"  ‚ö†Ô∏è Timeout - Retry {attempt + 1}/{max_retries} after {backoff}s")
                time.sleep(backoff)
            except requests.RequestException as e:
                print(f"  ‚ùå Request error: {e}")
                if attempt == max_retries - 1:
                    raise

        return None

    def normalizar_tipo(self, tipo: str) -> str:
        """Normalize publication type (case-insensitive, no accents)"""
        if not tipo:
            return ""
        # Remove accents
        sem_acentos = unicodedata.normalize('NFD', tipo)
        sem_acentos = ''.join(c for c in sem_acentos if unicodedata.category(c) != 'Mn')
        return sem_acentos.lower().strip()

    def extrair_ementa(self, texto: str) -> Optional[str]:
        """Extract ementa from text"""
        patterns = [
            r'EMENTA\s*[:\-]?\s*(.+?)(?=AC√ìRD√ÉO|VOTO|$)',
            r'E\s*M\s*E\s*N\s*T\s*A\s*[:\-]?\s*(.+?)(?=AC√ìRD√ÉO|VOTO|$)',
        ]

        for pattern in patterns:
            match = re.search(pattern, texto, re.IGNORECASE | re.DOTALL)
            if match:
                ementa = match.group(1).strip()
                return ementa[:2000] if len(ementa) > 2000 else ementa

        return None

    def extrair_relator(self, texto: str) -> Optional[str]:
        """Extract rapporteur/judge name"""
        patterns = [
            r'RELATOR\s*[:\-]?\s*(.+?)(?=\n|REQUERENTE)',
            r'Rel\.\s*[:\-]?\s*(.+?)(?=\n)',
            r'MINISTRO\s+([A-Z√Ä-√ö\s]+)',
            r'DESEMBARGADOR\s+([A-Z√Ä-√ö\s]+)',
        ]

        for pattern in patterns:
            match = re.search(pattern, texto, re.IGNORECASE)
            if match:
                return match.group(1).strip()[:200]

        return None

    def processar_publicacao(self, raw_data: Dict) -> Dict:
        """Process raw API data into structured publication"""
        # Clean HTML
        soup = BeautifulSoup(raw_data.get('texto', ''), 'html.parser')
        texto_limpo = soup.get_text(separator='\n', strip=True)

        # Generate hash for deduplication
        hash_conteudo = hashlib.sha256(texto_limpo.encode()).hexdigest()

        # Extract ementa
        ementa = self.extrair_ementa(texto_limpo)

        # Extract relator
        relator = self.extrair_relator(texto_limpo)

        # Normalize type
        tipo_raw = raw_data.get('tipoComunicacao', 'Intima√ß√£o')
        tipo_normalizado = self.normalizar_tipo(tipo_raw)

        # Classify
        if 'acordao' in texto_limpo.lower():
            tipo_publicacao = 'Ac√≥rd√£o'
        elif 'sentenca' in texto_limpo.lower():
            tipo_publicacao = 'Senten√ßa'
        elif 'decisao' in texto_limpo.lower():
            tipo_publicacao = 'Decis√£o'
        else:
            tipo_publicacao = tipo_raw or 'Intima√ß√£o'

        return {
            'id': str(uuid.uuid4()),
            'hash_conteudo': hash_conteudo,
            'numero_processo': raw_data.get('numero_processo'),
            'numero_processo_fmt': raw_data.get('numeroprocessocommascara'),
            'tribunal': raw_data.get('siglaTribunal'),
            'orgao_julgador': raw_data.get('nomeOrgao'),
            'tipo_publicacao': tipo_publicacao,
            'classe_processual': raw_data.get('nomeClasse'),
            'texto_html': raw_data.get('texto', ''),
            'texto_limpo': texto_limpo,
            'ementa': ementa,
            'data_publicacao': raw_data.get('data_disponibilizacao'),
            'relator': relator,
            'fonte': 'DJEN'
        }

    def inserir_publicacao(self, pub: Dict) -> bool:
        """Insert publication (returns True if new, False if duplicate)"""
        cursor = self.conn.cursor()

        try:
            cursor.execute("""
            INSERT INTO publicacoes (
                id, hash_conteudo, numero_processo, numero_processo_fmt,
                tribunal, orgao_julgador, tipo_publicacao, classe_processual,
                texto_html, texto_limpo, ementa, data_publicacao, relator, fonte
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                pub['id'], pub['hash_conteudo'], pub['numero_processo'],
                pub['numero_processo_fmt'], pub['tribunal'], pub['orgao_julgador'],
                pub['tipo_publicacao'], pub['classe_processual'], pub['texto_html'],
                pub['texto_limpo'], pub['ementa'], pub['data_publicacao'],
                pub['relator'], pub['fonte']
            ))
            return True
        except sqlite3.IntegrityError:
            # Duplicate (hash exists)
            return False

    def baixar_publicacoes(self, tribunal: str, data: str, max_pages: Optional[int] = None):
        """Download publications with pagination"""
        print(f"\nüì• Downloading publications: {tribunal} - {data}")

        page = 1
        total_items = 0

        while True:
            print(f"\n  Page {page}...")

            response = self.fazer_requisicao(
                f"{BASE_URL}/api/v1/comunicacao",
                params={
                    'siglaTribunal': tribunal,
                    'dataInicio': data,
                    'dataFim': data,
                    'page': page,
                    'limit': 100
                }
            )

            if not response:
                print(f"  ‚ùå Failed to fetch page {page}")
                break

            data_json = response.json()
            items = data_json.get('items', [])

            if not items:
                print(f"  ‚ÑπÔ∏è No more items")
                break

            print(f"  Processing {len(items)} items...")

            # Process items (batch commits every 100)
            BATCH_SIZE = 100
            for i, item in enumerate(items, start=1):
                self.stats['total'] += 1

                try:
                    pub = self.processar_publicacao(item)
                    if self.inserir_publicacao(pub):
                        self.stats['novas'] += 1
                    else:
                        self.stats['duplicadas'] += 1
                except Exception as e:
                    print(f"    ‚ö†Ô∏è Error processing item: {e}")
                    self.stats['erros'] += 1

                # Batch commit
                if i % BATCH_SIZE == 0:
                    self.conn.commit()

            # Final commit for this page
            self.conn.commit()

            total_items += len(items)
            print(f"  ‚úÖ Processed: {total_items} total | "
                  f"New: {self.stats['novas']} | "
                  f"Duplicates: {self.stats['duplicadas']}")

            # Check if should continue
            if max_pages and page >= max_pages:
                print(f"  ‚ÑπÔ∏è Reached max pages limit ({max_pages})")
                break

            total_pages = data_json.get('totalPages', page)
            if page >= total_pages:
                print(f"  ‚ÑπÔ∏è Reached last page ({total_pages})")
                break

            page += 1

    def print_summary(self):
        """Print final statistics"""
        print(f"\n" + "=" * 60)
        print(f"üìä DOWNLOAD SUMMARY")
        print(f"=" * 60)
        print(f"Total processed:  {self.stats['total']}")
        print(f"New insertions:   {self.stats['novas']}")
        print(f"Duplicates:       {self.stats['duplicadas']}")
        print(f"Errors:           {self.stats['erros']}")
        print(f"=" * 60)

    def close(self):
        """Close database connection"""
        if self.conn:
            self.conn.close()


def main():
    parser = argparse.ArgumentParser(
        description='DJEN API Download Example with Best Practices'
    )
    parser.add_argument('--tribunal', default='STJ', help='Tribunal code')
    parser.add_argument('--data', default=datetime.now().strftime('%Y-%m-%d'),
                        help='Publication date (YYYY-MM-DD)')
    parser.add_argument('--output', default='publicacoes.db',
                        help='Output SQLite database')
    parser.add_argument('--max-pages', type=int, default=None,
                        help='Maximum pages to download')

    args = parser.parse_args()

    print(f"üöÄ DJEN API Download Example")
    print(f"Tribunal: {args.tribunal}")
    print(f"Date: {args.data}")
    print(f"Output: {args.output}")

    downloader = DJENDownloader(args.output)

    try:
        start_time = time.time()

        downloader.inicializar_banco()
        downloader.baixar_publicacoes(args.tribunal, args.data, args.max_pages)
        downloader.print_summary()

        elapsed = time.time() - start_time
        print(f"\n‚è±Ô∏è Total time: {elapsed:.1f}s")

    finally:
        downloader.close()


if __name__ == '__main__':
    main()
